{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1a5a86-0ec3-4cb4-88df-1f6d320a980a",
   "metadata": {},
   "source": [
    "# 영어-한국어 번역기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a043e058-431e-4a7e-9917-f2096a27e36e",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b96c67-589e-40e6-a845-fb533d81a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd52a199-ca02-4fbb-919b-b3423472884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heerak/anaconda3/envs/heerak/lib/python3.8/site-packages/huggingface_hub/utils/_hf_folder.py:92: UserWarning: A token has been found in `/home/heerak/.huggingface/token`. This is the old path where tokens were stored. The new location is `/home/heerak/.cache/huggingface/token` which is configurable using `HF_HOME` environment variable. Your token has been copied to this new location. You can now safely delete the old token file manually or use `huggingface-cli logout`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6671c625cf4840928fb1f0c31a40e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/360 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /data/heerak/.cache/Heerak___parquet/Heerak--ko_en_translation-38d10c8228802b34/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c88298dbe1746c18cc4ae988bb8f049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d5106a4526437ebe724af725b2da9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/208M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbf8bbb98044b96ac2a1be42d8c509f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/352M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28af4a158e294e0793dbbc881449d9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/281M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbada9e1de34173b909ebb9e73a755c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/273M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e1e4e707ef4aad80aa449bd8d1445a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/296M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538eaceffa184bebb9803a89f0888be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/139M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7195e4e5e845a081eea63f5a547b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7459339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split:   0%|          | 0/740903 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /data/heerak/.cache/Heerak___parquet/Heerak--ko_en_translation-38d10c8228802b34/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49688e86c7f946dab097e7b1ca99c31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ko_en_dataset = load_dataset('Heerak/ko_en_translation', cache_dir='/data/heerak/.cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c8aa609-00ff-4379-bf91-cca9e7e5c268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ko', 'en', '__index_level_0__'],\n",
       "        num_rows: 7459339\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['ko', 'en', '__index_level_0__'],\n",
       "        num_rows: 740903\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_en_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "346de6cd-1fc5-4cdd-a6fe-9ae591075964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.', '형님 제일 웃긴 그림이 뭔지 알아요.', '속옷을?']\n"
     ]
    }
   ],
   "source": [
    "print(ko_en_dataset['train']['ko'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff6bb10b-b65f-4278-ae74-6f8e41f001a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If you reply to the color you want, we will start making it right away.', 'You know what the funniest picture is.', 'Underwear?']\n"
     ]
    }
   ],
   "source": [
    "print(ko_en_dataset['train']['en'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dd84119-f590-43d3-a8d2-bc24a3c9b191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f778a8d8ab774f0e96e5770fcf8ecfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/510 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /data/heerak/.cache/Heerak___parquet/Heerak--en_ko_translation-5921884cd7d8cfd7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03dd99abf60f4b379c2a4dc80a6a2308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74737a3456ae460191da6eac2154e3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/65.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0416b67c89f448a9e5de70feb0b3b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9939aabe90b54c89949dbf1098416ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/283M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5285e80652b948fd83a3e4824f6dad0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/172M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092ceb6b275a48269ee17f042e582f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/95.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93cc33d02059449aa224ae7a080d3113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3326179 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split:   0%|          | 0/403802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /data/heerak/.cache/Heerak___parquet/Heerak--en_ko_translation-5921884cd7d8cfd7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac50859c8a2426ca0af45bf5500b6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_ko_dataset = load_dataset('Heerak/en_ko_translation', cache_dir='/data/heerak/.cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7611d34-63f1-4364-95eb-b6f0c099274d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'ko', '__index_level_0__'],\n",
       "        num_rows: 3326179\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['en', 'ko', '__index_level_0__'],\n",
       "        num_rows: 403802\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_ko_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d5ce2cb-83b1-4478-a2ae-37eafa84670e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['그 말을 들으니 기쁘고, 저희와 거래하는 것을 고려해 주셨으면 합니다.', '확실히 생각하고 있습니다만, 몇 가지 여쭤보고 싶은 게 있어요.', '오늘날 세계 5가구 중 1가구는 고양이나 개 또는 둘 다를 키우고 있습니다.']\n"
     ]
    }
   ],
   "source": [
    "print(en_ko_dataset['train']['ko'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f1c442d-686f-4e79-b409-5cc341cd1b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm glad to hear that, and I hope you do consider doing business with us.\", \"I'm definitely thinking about it, but I have some queries to ask you.\", \"In today's world, one in every five families has either a cat, dog, or both.\"]\n"
     ]
    }
   ],
   "source": [
    "print(en_ko_dataset['train']['en'][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b1fe1-1810-45f7-9ffa-30761015c718",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bbf7a9e-ca81-4b42-bd34-acead0dfcefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba7a23c2-36ee-4302-8cdf-ba240f0c64ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ko_train_df = en_ko_dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f26b5733-896b-4821-9026-ee75266b5a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ko</th>\n",
       "      <th>__index_level_0__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm glad to hear that, and I hope you do consi...</td>\n",
       "      <td>그 말을 들으니 기쁘고, 저희와 거래하는 것을 고려해 주셨으면 합니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm definitely thinking about it, but I have s...</td>\n",
       "      <td>확실히 생각하고 있습니다만, 몇 가지 여쭤보고 싶은 게 있어요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In today's world, one in every five families h...</td>\n",
       "      <td>오늘날 세계 5가구 중 1가구는 고양이나 개 또는 둘 다를 키우고 있습니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When you tell them, we'll take care of their c...</td>\n",
       "      <td>그들에게 말하면, 저희가 그 아이들을 돌볼 것입니다.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK, how about for swimming?</td>\n",
       "      <td>좋아요, 수영은 어떤가요?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0  I'm glad to hear that, and I hope you do consi...   \n",
       "1  I'm definitely thinking about it, but I have s...   \n",
       "2  In today's world, one in every five families h...   \n",
       "3  When you tell them, we'll take care of their c...   \n",
       "4                        OK, how about for swimming?   \n",
       "\n",
       "                                           ko  __index_level_0__  \n",
       "0     그 말을 들으니 기쁘고, 저희와 거래하는 것을 고려해 주셨으면 합니다.                  0  \n",
       "1         확실히 생각하고 있습니다만, 몇 가지 여쭤보고 싶은 게 있어요.                  1  \n",
       "2  오늘날 세계 5가구 중 1가구는 고양이나 개 또는 둘 다를 키우고 있습니다.                  2  \n",
       "3               그들에게 말하면, 저희가 그 아이들을 돌볼 것입니다.                  3  \n",
       "4                              좋아요, 수영은 어떤가요?                  4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_ko_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3346f9e3-e516-411d-9ad3-8972eda3ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_en_train_df = ko_en_dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a9fa499-2801-4ba0-85b8-aa60a537628d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ko</th>\n",
       "      <th>en</th>\n",
       "      <th>__index_level_0__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.</td>\n",
       "      <td>If you reply to the color you want, we will st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>형님 제일 웃긴 그림이 뭔지 알아요.</td>\n",
       "      <td>You know what the funniest picture is.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>속옷을?</td>\n",
       "      <td>Underwear?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그래도 가격이 꽤 비싸니까 많이 살게요.</td>\n",
       "      <td>I wont buy a lot though since the price is sti...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAA님, 제가 회의에서 화를 냈던 점 정말 사과드리고 싶습니다.</td>\n",
       "      <td>Dear AAA, I really want to apologize for my an...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ko  \\\n",
       "0       원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.   \n",
       "1                  형님 제일 웃긴 그림이 뭔지 알아요.   \n",
       "2                                  속옷을?   \n",
       "3                그래도 가격이 꽤 비싸니까 많이 살게요.   \n",
       "4  AAA님, 제가 회의에서 화를 냈던 점 정말 사과드리고 싶습니다.   \n",
       "\n",
       "                                                  en  __index_level_0__  \n",
       "0  If you reply to the color you want, we will st...                  0  \n",
       "1             You know what the funniest picture is.                  1  \n",
       "2                                         Underwear?                  2  \n",
       "3  I wont buy a lot though since the price is sti...                  3  \n",
       "4  Dear AAA, I really want to apologize for my an...                  4  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_en_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c13494f-713a-4cd8-bf11-2883854f3a76",
   "metadata": {},
   "source": [
    "### Make English Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f816be1-6b24-4669-91cd-119fbd394905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/heerak/workspace/translation_koenja/data'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.path.join(os.getcwd(), 'data')\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98c5e203-89b4-4377-aad8-3c91a472355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(data_path, 'vocab')):\n",
    "    os.makedirs(os.path.join(data_path, 'vocab'), exist_ok=True)\n",
    "    \n",
    "# vocab 을 만들기 위한 text file 만듦\n",
    "with open(os.path.join(data_path, 'vocab', 'english.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(en_ko_train_df['en']))\n",
    "    f.write('\\n')\n",
    "    f.write('\\n'.join(ko_en_train_df['en']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bed22ed5-4f60-4c47-b1ff-22a66234f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86a67b3a-8123-43aa-af06-bacca7a17a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(\n",
    "    strip_accents=False,  # True 일 경우 악센트 제거. ex) é → e, ô → o\n",
    "    lowercase=False,  # 대소문자 구분 여부. True 일 경우 구분하지 않음\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e039d36e-d142-43da-b562-2e3191de0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 학습시 필요한 parameter\n",
    "data_file = os.path.join(data_path, 'vocab', 'english.txt')\n",
    "vocab_size = 32000\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 5\n",
    "\n",
    "special_tokens = ['[PAD]', '[UNK]', '[SOS]', '[EOS]', '[MASK]']\n",
    "\n",
    "unused_ids = 200\n",
    "unused_special_token_list = ['[UNUSED{}]'.format(i) for i in range(unused_ids)]\n",
    "special_tokens.extend(unused_special_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acac4e9b-0d9b-4dae-bc62-e165e0b5759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(\n",
    "    files=data_file,  # 단어 집합을 얻기 위해 학습할 데이터\n",
    "    vocab_size=vocab_size,  # 단어 집합의 크기\n",
    "    min_frequency=min_frequency,  # 최소 해당 횟수만큼 등장한 쌍(pair)의 경우에만 병합 대상이 됨\n",
    "    limit_alphabet=limit_alphabet,  # 병합 전의 초기 토큰의 허용 개수\n",
    "    special_tokens=special_tokens,  # special token 리스트\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e579aba-9883-4df1-ac32-06230303f796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=32000, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=False, lowercase=False, wordpieces_prefix=##)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b90a690-7f28-41e4-bb20-f288a3025d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tokenizer/english/vocab.txt']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists('./tokenizer/english'):\n",
    "    os.makedirs('./tokenizer/english', exist_ok=True)\n",
    "tokenizer.save_model('./tokenizer/english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f35793c-db8a-45c6-9314-796d91cf1283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과 : ['I', 'love', 'you']\n",
      "정수 인코딩 : [244, 7869, 4153]\n",
      "디코딩 : I love you\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('I love you')\n",
    "print('토큰화 결과 :', encoded.tokens)\n",
    "print('정수 인코딩 :', encoded.ids)\n",
    "print('디코딩 :', tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9edec-6ade-43c1-875d-00196b97bd49",
   "metadata": {},
   "source": [
    "### Make Korean Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98faae5c-4898-47c2-ba61-afec6a547968",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(data_path, 'vocab')):\n",
    "    os.makedirs(os.path.join(data_path, 'vocab'), exist_ok=True)\n",
    "    \n",
    "# vocab 을 만들기 위한 text file 만듦\n",
    "with open(os.path.join(data_path, 'vocab', 'korean.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(en_ko_train_df['ko']))\n",
    "    f.write('\\n')\n",
    "    f.write('\\n'.join(ko_en_train_df['ko']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2ccb27a-11c1-4d21-ac7e-48f938d28da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(\n",
    "    strip_accents=False,  # True 일 경우 악센트 제거. ex) é → e, ô → o\n",
    "    lowercase=False,  # 대소문자 구분 여부. True 일 경우 구분하지 않음\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c3313d3-e15f-42c1-9b26-a83d13eb482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 학습시 필요한 parameter\n",
    "data_file = os.path.join(data_path, 'vocab', 'korean.txt')\n",
    "vocab_size = 32000\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 5\n",
    "\n",
    "special_tokens = ['[PAD]', '[UNK]', '[SOS]', '[EOS]', '[MASK]']\n",
    "\n",
    "unused_ids = 200\n",
    "unused_special_token_list = ['[UNUSED{}]'.format(i) for i in range(unused_ids)]\n",
    "special_tokens.extend(unused_special_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34ee4277-f4ba-450f-a157-47e856b6d3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(\n",
    "    files=data_file,  # 단어 집합을 얻기 위해 학습할 데이터\n",
    "    vocab_size=vocab_size,  # 단어 집합의 크기\n",
    "    min_frequency=min_frequency,  # 최소 해당 횟수만큼 등장한 쌍(pair)의 경우에만 병합 대상이 됨\n",
    "    limit_alphabet=limit_alphabet,  # 병합 전의 초기 토큰의 허용 개수\n",
    "    special_tokens=special_tokens,  # special token 리스트\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a60494fa-3167-43bd-99ff-54eb1c6142b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=32000, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=False, lowercase=False, wordpieces_prefix=##)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c0bb0c5-743a-4975-816b-4659234b937b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tokenizer/korean/vocab.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists('./tokenizer/korean'):\n",
    "    os.makedirs('./tokenizer/korean', exist_ok=True)\n",
    "tokenizer.save_model('./tokenizer/korean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2ad73f7-72eb-4ddb-948f-398dbf2c4cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과 : ['오늘', '##도', '손흥', '##민을', '보며', '하루', '##를', '시작하고', '토트넘', '우승을', '향해', '응원', '##하자', '!']\n",
      "정수 인코딩 : [9859, 6232, 17485, 19984, 28345, 10673, 6228, 28302, 25006, 23108, 12743, 19585, 11833, 205]\n",
      "디코딩 : 오늘도 손흥민을 보며 하루를 시작하고 토트넘 우승을 향해 응원하자!\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('오늘도 손흥민을 보며 하루를 시작하고 토트넘 우승을 향해 응원하자!')\n",
    "print('토큰화 결과 :', encoded.tokens)\n",
    "print('정수 인코딩 :', encoded.ids)\n",
    "print('디코딩 :', tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ada72-661b-4fcb-8fd2-51af5dda8456",
   "metadata": {},
   "source": [
    "## Seed Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "728351a9-37f4-4efe-81cf-636114a4126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fcea57b-b10e-453c-8c30-6d8d7f412e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee58b39-9563-4604-8ec9-3e6943ed3b73",
   "metadata": {},
   "source": [
    "## Model Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32f402-57b8-4605-9112-54b829f3d022",
   "metadata": {},
   "source": [
    "### Vanila Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65425c8c-d906-4f09-af60-f9e89f85ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce263319-7d69-4ef7-9cad-419b282904fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(-torch.arange(0, d_model, 2) * math.log(10000) / d_model)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, d_model))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)  # 일종의 layer 로서 작용하지만, optimizer 에 의해 업데이트 되지 않도록 함\n",
    "        \n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.d_model)\n",
    "    \n",
    "    \n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 2048,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=d_model, \n",
    "                                          nhead=nhead, \n",
    "                                          num_encoder_layers=num_encoder_layers, \n",
    "                                          num_decoder_layers=num_decoder_layers, \n",
    "                                          dim_feedforward=dim_feedforward, \n",
    "                                          dropout=dropout)\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, d_model)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                tgt: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        \n",
    "        # 토큰 임베딩을 거친 후 positional encoding 을 수행하여 임베딩 벡터를 만듦\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        \n",
    "        # Seq2Seq 트랜스포머 네트워크를 통과함\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        \n",
    "        # 디코더 결과를 linear layer 에 통과시켜 target vocab size 만큼 차원을 맞추어 줌\n",
    "        return self.linear(outs)\n",
    "    \n",
    "    def encode(self, \n",
    "               src: Tensor,\n",
    "               src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)\n",
    "    \n",
    "    def decode(self, \n",
    "               tgt: Tensor,\n",
    "               memory: Tensor,\n",
    "               tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c7101-691f-43ad-b7c5-9ee1715a288a",
   "metadata": {},
   "source": [
    "### Source(english) Tokenizer | Target(Korean) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61dba428-2000-4372-b4a7-60d90bd2f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9f47806-092c-4da2-9df0-60402d115050",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab_path = \"./tokenizer/english/\"\n",
    "source_tokenizer = BertTokenizerFast.from_pretrained(source_vocab_path,\n",
    "                                                     unk_token='[UNK]',\n",
    "                                                     sep_token='[EOS]',\n",
    "                                                     pad_token='[PAD]',\n",
    "                                                     cls_token='[SOS]',\n",
    "                                                     mask_token='[MASK]',\n",
    "                                                     model_max_length=256)\n",
    "\n",
    "target_vocab_path = \"./tokenizer/korean/\"\n",
    "target_tokenizer = BertTokenizerFast.from_pretrained(target_vocab_path,\n",
    "                                                     unk_token='[UNK]',\n",
    "                                                     sep_token='[EOS]',\n",
    "                                                     pad_token='[PAD]',\n",
    "                                                     cls_token='[SOS]',\n",
    "                                                     mask_token='[MASK]',\n",
    "                                                     model_max_length=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587bf96a-1851-46eb-8beb-70a80d583b69",
   "metadata": {},
   "source": [
    "## DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "caf5e876-bc9c-449f-aa6e-e4632123477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerBase\n",
    "from transformers.tokenization_utils_base import PaddingStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c5e1f58-57cd-444e-8fea-d648ece54c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataCollator:\n",
    "\n",
    "    def __init__(self, is_test=False):\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        b_src = []\n",
    "        if not self.is_test:\n",
    "            b_tgt = []\n",
    "            b_labels = []\n",
    "        \n",
    "        for b in batch:\n",
    "            b_src.append(b['src_input_ids'])\n",
    "            \n",
    "            if not self.is_test:\n",
    "                b_tgt.append(b['tgt_input_ids'])\n",
    "                b_labels.append(b['labels'])\n",
    "                \n",
    "        t_src = torch.LongTensor(b_src)  # List[Tensor] -> Tensor List\n",
    "        if self.is_test:\n",
    "            return {'src': t_src}\n",
    "        else:\n",
    "            t_tgt = torch.LongTensor(b_tgt)\n",
    "            t_labels = torch.LongTensor(b_labels)\n",
    "            return {'src': t_src, 'tgt': t_tgt, 'labels': b_labels}\n",
    "        \n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28b8eed-f7e7-47b6-812a-5522778ca9c2",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1649228-5bda-4edd-b3dc-f3290578e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65f45a4b-b1b1-4693-90d6-dcdbca531116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, src_lang, tgt_lang, src_tokenizer, tgt_tokenizer):\n",
    "    train_dataset = dataset['train']\n",
    "    valid_dataset = dataset['valid']\n",
    "    \n",
    "    data_collator = TransformerDataCollator()\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        inputs = [ex for ex in examples[src_lang]]\n",
    "        targets = [ex for ex in examples[tgt_lang]]\n",
    "        tokenized_inputs = src_tokenizer(inputs, max_length=256, padding='max_length', truncation=True)\n",
    "\n",
    "        # Tokenize targets with the `text_target` keyword argument\n",
    "        tokenized_targets = tgt_tokenizer(text_target=targets, max_length=256, padding='max_length', truncation=True)\n",
    "\n",
    "        result = {}\n",
    "        result['src_input_ids'] = tokenized_inputs['input_ids']\n",
    "        result['tgt_input_ids'] = tokenized_targets['input_ids']\n",
    "        \n",
    "        # if we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "        # padding in the loss.\n",
    "        result['labels'] = [[(t if t != tgt_tokenizer.pad_token_id else -100) for t in tokenized_target] \n",
    "                            for tokenized_target in tokenized_targets['input_ids']]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    train_dataset = train_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "    )\n",
    "    valid_dataset = valid_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=valid_dataset.column_names,\n",
    "    )\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  collate_fn=data_collator,\n",
    "                                  shuffle=True,\n",
    "                                  batch_size=batch_size)\n",
    "    \n",
    "    valid_dataloader = DataLoader(valid_dataset,\n",
    "                                  collate_fn=data_collator,\n",
    "                                  shuffle=False,\n",
    "                                  batch_size=batch_size)\n",
    "    \n",
    "    return {'train': train_dataloader,\n",
    "            'valid': valid_dataloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65a559a-ec25-43e1-ac64-7f4e4e3af0e6",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35cd19da-792d-4969-840c-9caef1bb10d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, optimizer, loss_fn, batch_size, src_lang, tgt_lang, device, src_tokenizer, tgt_tokenizer, dataset):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.device = device\n",
    "        \n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        \n",
    "        dataloader = get_dataloader(dataset, src_lang, tgt_lang, src_tokenizer, tgt_tokenizer)\n",
    "        \n",
    "        self.train_dataloader = dataloader['train']\n",
    "        self.valid_dataloader = dataloader['valid']\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        losses = 0\n",
    "        \n",
    "        for batch in tqdm(self.train_dataloader):\n",
    "            src = batch['src'].to(self.device)\n",
    "            tgt = batch['tgt'].to(self.device)\n",
    "            \n",
    "            tgt_input = tgt[:-1, :]  # batch 에 있는 단어에서 마지막 [EOS] 토큰 제거\n",
    "            \n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, tgt_input)\n",
    "            \n",
    "            logits = self.model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            tgt_out = tgt[1:, :]  # batch 안에 있는 단어에서 [SOS] 토큰을 제거한 정답 sequence\n",
    "            \n",
    "            # Transformer Network 의 teacher forcing 을 아래와 같이 구현함\n",
    "            # 즉, 학습시 auto regressive 한 디코더의 특징을 활용하는 것이 아니라,\n",
    "            # t-1 시점의 예측과 무관히 t 시점 디코더에는 ground truth 토큰을 입력함\n",
    "            \n",
    "            # tgt_input 은 [SOS] 토큰으로 '정답 sequence 인 tgt_out 의 첫 토큰'을 맞추려 함\n",
    "            # tgt_input 의 마지막 토큰으로 '정답 sequence 인 tgt_out 의 마지막 토큰'인 [EOS] 를 맞추려 함\n",
    "            loss = self.loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            losses += loss.item()\n",
    "            \n",
    "        return losses / len(self.train_dataloader)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        losses = 0\n",
    "        \n",
    "        for batch in tqdm(self.valid_dataloader):\n",
    "            with torch.no_grad():\n",
    "                src = batch['src'].to(self.device)\n",
    "                tgt = batch['tgt'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "\n",
    "                tgt_input = tgt[:-1, :]  # batch 에 있는 단어에서 마지막 [EOS] 토큰 제거\n",
    "\n",
    "                src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, tgt_input)\n",
    "\n",
    "                logits = self.model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "                tgt_out = labels[1:, :]  # batch 안에 있는 단어에서 [SOS] 토큰을 제거한 정답 sequence\n",
    "\n",
    "                # tgt_input 은 [SOS] 토큰으로 '정답 sequence 인 tgt_out 의 첫 토큰'을 맞추려 함\n",
    "                # tgt_input 의 마지막 토큰으로 '정답 sequence 인 tgt_out 의 마지막 토큰'인 [EOS] 를 맞추려 함\n",
    "                loss = self.loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "                losses += loss.item()\n",
    "            \n",
    "            # valid 에 대해서는 backward propagation 을 수행하지 않음\n",
    "        \n",
    "        return losses / len(self.valid_dataloader)\n",
    "            \n",
    "            \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones((sz, sz), device=self.device)) == 1).transpose(0, 1)\n",
    "        \n",
    "        \"\"\"\n",
    "        (eg.) 타겟 시퀀스 길이가 5 토큰이라고 가정하면,\n",
    "        >>> mask\n",
    "        tensor([[True, False, False, False, False],  # 첫 토큰은 자기 자신만 attend 할 수 있음\n",
    "                [True,  True, False, False, False],\n",
    "                [True,  True,  True, False, False],\n",
    "                [True,  True,  True,  True, False],\n",
    "                [True,  True,  True,  True,  True]])  # 마지막 토큰은 모든 토큰을 attend 할 수 있음\n",
    "        \"\"\"\n",
    "        \n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        \"\"\"\n",
    "        (eg.) 타겟 시퀀스 길이가 5 토큰이라고 가정하면,\n",
    "        >>> mask\n",
    "        tensor([[0, -inf, -inf, -inf, -inf],  # 첫 토큰은 자기 자신만 attend 할 수 있음\n",
    "                [0,    0, -inf, -inf, -inf],\n",
    "                [0,    0,    0, -inf, -inf],\n",
    "                [0,    0,    0,    0, -inf],\n",
    "                [0,    0,    0,    0,    0]])  # 마지막 토큰은 모든 토큰을 attend 할 수 있음\n",
    "        \"\"\"\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "            \n",
    "    def create_mask(self, src, tgt):\n",
    "        \"\"\" mask 되지 않고 attend 할 수 있는 토큰을 False 또는 0 값으로 이루어지도록 처리함 \"\"\"\n",
    "        src_seq_len = src.shape[0]\n",
    "        tgt_seq_len = tgt.shape[0]\n",
    "        \n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n",
    "        src_mask = torch.zeros((src_seq_len, src_seq_len), device=self.device).type(torch.bool)\n",
    "        # src_mask 는 (src 시퀀스길이 X src 시퀀스길이)의 False 들로 이루어진 행렬임)\n",
    "        \n",
    "        src_padding_mask = (src == self.src_tokenizer.pad_token_id).transpose(0, 1)\n",
    "        tgt_padding_mask = (tgt == self.tgt_tokenizer.pad_token_id).transpose(0, 1)\n",
    "        \n",
    "        \"\"\"\n",
    "        (eg.)\n",
    "        src_padding_mask 및 tgt_padding_mask 는 아래와 같이 batch 로 묶인 데이터들에 대해서 pad_token_id 에 해당하는 부분은 False 로 처리함\n",
    "        \n",
    "        tensor([[False, False, False, False, False, True, True, True, True],\n",
    "                [False, False, False, False, True, True, True, True, True],\n",
    "                [False, False, False, False, False, False, False, False, True]])\n",
    "        \"\"\"\n",
    "        \n",
    "        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92394f6c-e61f-4385-ba0b-39661f75d114",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e266eb1d-4bbc-4a86-829b-c7bf6a682f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Train Parameter\n",
    "num_epochs = 30\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "dim_feedforward = 2048\n",
    "batch_size = 16\n",
    "src_vocab_size = source_tokenizer.vocab_size\n",
    "tgt_vocab_size = target_tokenizer.vocab_size\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4698db5-a666-43de-93dd-423a8ed535c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67b70d79-8980-485b-9ab1-a9803f4d716b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=32000, bias=True)\n",
       "  (src_tok_emb): TokenEmbedding(\n",
       "    (embedding): Embedding(32000, 512)\n",
       "  )\n",
       "  (tgt_tok_emb): TokenEmbedding(\n",
       "    (embedding): Embedding(32000, 512)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = Seq2SeqTransformer(num_encoder_layers=num_encoder_layers, \n",
    "                                 num_decoder_layers=num_decoder_layers, \n",
    "                                 d_model=d_model, \n",
    "                                 nhead=nhead, \n",
    "                                 src_vocab_size=src_vocab_size, \n",
    "                                 tgt_vocab_size=tgt_vocab_size,\n",
    "                                 dim_feedforward=dim_feedforward)\n",
    "transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "594526a2-b0da-4c2a-80d8-e07fc1b387c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xavier_uniform 파라미터 초기화\n",
    "for n, p in transformer.named_parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf110d03-5222-45e7-a5d3-a5f0870da0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04678eb0-71a1-48b9-8c17-73f0bb0c8c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3326179 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/403802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(model=transformer,\n",
    "                  optimizer=optimizer,\n",
    "                  loss_fn=loss_fn, \n",
    "                  batch_size=batch_size,\n",
    "                  src_lang='en',\n",
    "                  tgt_lang='ko',\n",
    "                  device=device,\n",
    "                  src_tokenizer=source_tokenizer,\n",
    "                  tgt_tokenizer=target_tokenizer,\n",
    "                  dataset=en_ko_dataset,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d7525e4-fa58-4939-b7cf-c726c6b48700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39ef7d39-2fbf-4d0f-9598-a7ecac513485",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './log/transformer'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "save_dir = './saved_model/transformer'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "634b256c-885d-4842-ad37-36c249e46dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                  | 87319/207887 [6:30:43<7:22:55,  4.54it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 49%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                 | 102467/207887 [7:26:09<6:25:05,  4.56it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 56%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                 | 117211/207887 [8:16:38<5:03:18,  4.98it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 64%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                 | 132275/207887 [9:07:02<4:12:27,  4.99it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 71%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                 | 147319/207887 [9:57:06<3:25:27,  4.91it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                 | 161456/207887 [10:44:42<2:43:34,  4.73it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 176130/207887 [11:34:48<1:48:50,  4.86it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                  | 190755/207887 [12:25:12<1:00:19,  4.73it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 205274/207887 [13:15:32<09:22,  4.64it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    start_time = timer()\n",
    "    train_loss = trainer.train_epoch()\n",
    "    end_time = timer()\n",
    "    \n",
    "    valid_loss = trainer.evaluate()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    print(f'Epoch: {epoch}, Train loss: {train_loss:.3f}, Valid loss: {valid_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s')\n",
    "    \n",
    "    # 에폭마다 loss 의 History 를 남김\n",
    "    np.savetxt(os.path.join(log_dir, 'loss_history.txt'), np.array([train_losses, valid_losses]), fmt='%.4e')\n",
    "    \n",
    "    if epoch == 1:\n",
    "        LEAST_VALID_LOSS = valid_loss\n",
    "        torch.save({'model': transformer.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()},\n",
    "                   os.path.join(save_dir, 'best_model.pt'))\n",
    "        source_tokenizer.save_pretrained(save_dir)\n",
    "        target_tokenizer.save_pretrained(save_dir)\n",
    "    else:\n",
    "        if LEAST_VALID_LOSS > valid_loss:\n",
    "            print('모델 갱신 : valid loss {}'.format(valid_loss))\n",
    "            LEAST_VALID_LOSS = valid_loss\n",
    "        torch.save({'model': transformer.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()},\n",
    "                   os.path.join(save_dir, 'best_model.pt'))\n",
    "        source_tokenizer.save_pretrained(save_dir)\n",
    "        target_tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "efeadc24-37fe-495e-b076-8230ba64276d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697cdad8-9b80-4f37-b453-fb7f9d7c7025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_heerak",
   "language": "python",
   "name": "conda_heerak"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
